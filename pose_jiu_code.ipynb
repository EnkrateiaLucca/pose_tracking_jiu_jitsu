{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installing the requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvQIfrzE25Hu",
        "outputId": "d62fa410-a300-45ce-e431-4e7c76ca9c97"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.19.3\n",
        "!pip install mediapipe\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install natsorted\n",
        "!pip install pandas\n",
        "!pip install plotly\n",
        "!pip install seaborn"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up automatic reload for the scripts that will be written outside the jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzUeDmrg4xGU"
      },
      "outputs": [],
      "source": [
        "from base64 import b64encode\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "from typing import List # I don't think I need this!\n",
        "\n",
        "# Custom imports\n",
        "from pose_tracking_utils import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up all the necessary variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "VIDEO_PATH = \"./videos/clip_training_session_2.mp4\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMAKyQ893kSr",
        "outputId": "fca8834b-9067-4e39-ea50-a60077199030"
      },
      "source": [
        "Now we create the pose tracking video to visualize the joints and connections."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compress the video and visualize it on jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For webcam input:\n",
        "output_path = create_pose_tracking_video(VIDEO_PATH)\n",
        "#compressed_path = save_compressed_video(output_path)\n",
        "compressed_path = save_compressed_video(output_path)\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the landmark 3D plot animation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wk_RmGfWzg4",
        "outputId": "838fcb67-df47-41e2-b353-1e0d5a62771d"
      },
      "outputs": [],
      "source": [
        "anim_output_path = f\"./{VIDEO_PATH[:-4]}_anim_landmarks3D.mp4\"\n",
        "create_landmarks_plot3D_animation(VIDEO_PATH, anim_output_path)\n",
        "\n",
        "compressed_path = save_compressed_video(anim_output_path)\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pose landmarks are this:\n",
        "\n",
        "Another list of pose landmarks in world coordinates. Each landmark consists of the following:\n",
        "\n",
        "x, y and z: Real-world 3D coordinates in meters with the origin at the center between hips.\n",
        "visibility: Identical to that defined in the corresponding\n",
        "\n",
        "[source](https://google.github.io/mediapipe/solutions/pose.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the reference plot against which we will set objectives to improve\n",
        "the desired movements. Like for example I am using olympiam judokas\n",
        "as my reference to improve my uchimata movement.\n",
        "\n",
        "(*This function and the interactive widget are not finished*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pose_coords = get_pose_coords(video_path=VIDEO_PATH)\n",
        "# remove all nones from a list\n",
        "pose_coords = [x for x in pose_coords if x is not None]\n",
        "print(\"Length of the list with the pose coordinates: \",len(pose_coords))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's plot the x,y,z coordinates separately in time: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "right_foot_coords = [pose_coords[i].landmark[31] for i in range(len(pose_coords))]\n",
        "# Extract x, y, z coordinates from right and right foot\n",
        "x_coords = [coord.x for coord in right_foot_coords]\n",
        "y_coords = [coord.y for coord in right_foot_coords]\n",
        "z_coords = [coord.z for coord in right_foot_coords]\n",
        "\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_title('Foot Movement Over Time')\n",
        "ax.scatter(x_coords, y_coords, z_coords);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create 3 separate plots showing each coordinates in time\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 10))\n",
        "axs[0].plot(x_coords)\n",
        "axs[0].set_title('X Coordinate')\n",
        "axs[1].plot(y_coords)\n",
        "axs[1].set_title('Y Coordinate')\n",
        "axs[2].plot(z_coords)\n",
        "axs[2].set_title('Z Coordinate')\n",
        "plt.tight_layout();"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we create a 3D plot animation for body parts (in this case for the feet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_coords = [coord.x for coord in left_foot_coords]\n",
        "# y_coords = [coord.y for coord in left_foot_coords]\n",
        "# z_coords = [coord.z for coord in left_foot_coords]\n",
        "\n",
        "# x_coords_right = [coord.x for coord in right_foot_coords]\n",
        "# y_coords_right = [coord.y for coord in right_foot_coords]\n",
        "# z_coords_right = [coord.z for coord in right_foot_coords]\n",
        "\n",
        "# # Create a 3D scatter plot\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(111, projection='3d')\n",
        "# ax.set_xlabel('X')\n",
        "# ax.set_ylabel('Y')\n",
        "# ax.set_zlabel('Z')\n",
        "# ax.set_title('Foot Movement Over Time')\n",
        "\n",
        "# def update(frame):\n",
        "#     ax.clear()\n",
        "#     ax.set_xlabel('X')\n",
        "#     ax.set_ylabel('Y')\n",
        "#     ax.set_zlabel('Z')\n",
        "#     ax.set_title('Foot Movement Over Time')\n",
        "#     ax.scatter(x_coords[:frame], y_coords[:frame], z_coords[:frame])\n",
        "#     ax.scatter(x_coords_right[:frame], y_coords_right[:frame], z_coords_right[:frame])\n",
        "\n",
        "# ani = FuncAnimation(fig, update, frames=len(x_coords), interval=5)\n",
        "# ani.save('animation.mp4', writer='ffmpeg', fps=30)\n",
        "# #plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nicer looking landdmark plot of specific moments of the video, using plotly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting the pose coordinates\n",
        "[pose_coords[i].landmark[0].y for i in range(len(pose_coords))]\n",
        "#plot_landmarks(pose_coords[200].pose_landmarks,  mp_pose.POSE_CONNECTIONS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating a trace visualization\n",
        "VIDEO_PATH = \"./videos/uchimata_wall.mp4\"\n",
        "create_joint_trace(VIDEO_PATH,31, color_rgb=(0,255,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(VIDEO_PATH)\n",
        "create_joint_trace_video(VIDEO_PATH,31, color_rgb=(255,0,0))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realtime updating line plot of the x,y,z coordinates of the body parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Pose model\n",
        "body_part_index = 32\n",
        "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# Initialize OpenCV VideoCapture object to capture video from the camera\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "\n",
        "# Create an empty list to store the trace of the right elbow\n",
        "trace = []\n",
        "\n",
        "# Create empty lists to store the x, y, z coordinates of the right elbow\n",
        "x_vals = []\n",
        "y_vals = []\n",
        "z_vals = []\n",
        "\n",
        "# Create a Matplotlib figure and subplot for the real-time updating plot\n",
        "# fig, ax = plt.subplots()\n",
        "# plt.title('Time Lapse of the X Coordinate')\n",
        "# plt.xlabel('Frames')\n",
        "# plt.ylabel('Coordinate Value')\n",
        "# plt.xlim(0,1)\n",
        "# plt.ylim(0,1)\n",
        "# plt.ion()\n",
        "# plt.show()\n",
        "frame_num = 0\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the video capture\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "    # Convert the frame to RGB format\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Process the frame with MediaPipe Pose model\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # Check if any body parts are detected\n",
        "    \n",
        "    if results.pose_landmarks:\n",
        "        # Get the x,y,z coordinates of the right elbow\n",
        "        x, y, z = results.pose_landmarks.landmark[body_part_index].x, results.pose_landmarks.landmark[body_part_index].y, results.pose_landmarks.landmark[body_part_index].z\n",
        "        \n",
        "        # Append the x, y, z values to the corresponding lists\n",
        "        #x_vals.append(x)\n",
        "        y_vals.append(y)\n",
        "        #z_vals.append(z)\n",
        "        \n",
        "        # # Add the (x, y) coordinates to the trace list\n",
        "        trace.append((int(x * image.shape[1]), int(y * image.shape[0])))\n",
        "\n",
        "        # Draw the trace on the image\n",
        "        for i in range(len(trace)-1):\n",
        "            cv2.line(image, trace[i], trace[i+1], (255, 0, 0), thickness=2)\n",
        "\n",
        "        plt.title('Time Lapse of the Y Coordinate')\n",
        "        plt.xlabel('Frames')\n",
        "        plt.ylabel('Coordinate Value')\n",
        "        plt.xlim(0,len(pose_coords))\n",
        "        plt.ylim(0,1)\n",
        "        plt.plot(y_vals);\n",
        "        # Clear the plot and update with the new x, y, z coordinate values\n",
        "        #ax.clear()\n",
        "        # ax.plot(range(0, frame_num + 1), x_vals, 'r.', label='x')\n",
        "        # ax.plot(range(0, frame_num + 1), y_vals, 'g.', label='y')\n",
        "        # ax.plot(range(0, frame_num + 1), z_vals, 'b.', label='z')\n",
        "        # ax.legend(loc='upper left')\n",
        "        # plt.draw()\n",
        "        plt.pause(0.00000000001)\n",
        "        clear_output(wait=True)\n",
        "        frame_num += 1\n",
        "    \n",
        "    # Convert the image back to BGR format for display\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Display the image\n",
        "    cv2.imshow('Pose Tracking', image)\n",
        "\n",
        "    # Wait for user input to exit\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "    \n",
        "\n",
        "# Release the video capture, close all windows, and clear the plot\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_landmarks_plot3D_animation(\"videos/clip_training_session_2.mp4\", \n",
        "                                  \"videos/clip_training_session_2_landmarks3D.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignoring empty camera frame.\n",
            "Joint Trace graph created!\n",
            "Creating animation\n"
          ]
        }
      ],
      "source": [
        "def create_joint_trace_graph(video_path, output_path, body_part_index,xmin=300,xmax=1000,\n",
        "                             ymin=200,ymax=800):\n",
        "    \"\"\"\n",
        "    Creates a graph with the tracing of a particular body part,\n",
        "    while executing a certain movement.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "\n",
        "    # Create an empty list to store the trace of the body part being tracked\n",
        "    trace = []\n",
        "    i = 0\n",
        "    with mp_pose.Pose(min_detection_confidence=0.5,\n",
        "                    min_tracking_confidence=0.5) as pose:\n",
        "        while cap.isOpened():\n",
        "            success, image = cap.read()\n",
        "            if not success:\n",
        "                print(\"Ignoring empty camera frame.\")\n",
        "                break\n",
        "\n",
        "            # Convert the frame to RGB format\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            # Process the frame with MediaPipe Pose model\n",
        "            results = pose.process(image)\n",
        "\n",
        "            # Check if any body parts are detected\n",
        "            if results.pose_landmarks:\n",
        "                # Get the x,y coordinates of the body part being tracked (in this case, the right elbow)\n",
        "                x, y = int(results.pose_landmarks.landmark[body_part_index].x * image.shape[1]), int(results.pose_landmarks.landmark[body_part_index].y * image.shape[0])\n",
        "\n",
        "                # Add the coordinates to the trace list\n",
        "                trace.append((x, y))\n",
        "\n",
        "                # Plot the trace on the graph\n",
        "                fig, ax = plt.subplots()\n",
        "                #ax.imshow(image)\n",
        "                ax.set_xlim(xmin,xmax)\n",
        "                ax.set_ylim(ymin,ymax)\n",
        "                ax.invert_yaxis()\n",
        "                ax.plot(np.array(trace)[:, 0], np.array(trace)[:, 1], color='r')\n",
        "                plt.savefig(f'joint_trace{i}.png')\n",
        "                plt.close()\n",
        "                i+=1\n",
        "                #plt.pause(0.00000000001)\n",
        "                #clear_output(wait=True)\n",
        "                # Display the graph\n",
        "                #plt.show()\n",
        "            \n",
        "            if cv2.waitKey(5) & 0xFF == 27:\n",
        "                break\n",
        "        \n",
        "        cap.release()\n",
        "        print(\"Joint Trace graph created!\")\n",
        "        print(\"Creating animation\")\n",
        "        create_animation_from_png(\".\", output_path)\n",
        "\n",
        "\n",
        "video_path = \"./videos/uchimata_wall.mp4\"\n",
        "output_path = f\"{video_path[:-4]}_trace.mp4\"\n",
        "body_part_index = 31\n",
        "create_joint_trace_graph(video_path,output_path, body_part_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ffmpeg version 4.3 Copyright (c) 2000-2020 the FFmpeg developers\n",
            "  built with gcc 7.3.0 (crosstool-NG 1.23.0.449-a04d0)\n",
            "  configuration: --prefix=/home/lucassoares/anaconda3/envs/explore --cc=/opt/conda/conda-bld/ffmpeg_1597178665428/_build_env/bin/x86_64-conda_cos6-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame\n",
            "  libavutil      56. 51.100 / 56. 51.100\n",
            "  libavcodec     58. 91.100 / 58. 91.100\n",
            "  libavformat    58. 45.100 / 58. 45.100\n",
            "  libavdevice    58. 10.100 / 58. 10.100\n",
            "  libavfilter     7. 85.100 /  7. 85.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  7.100 /  5.  7.100\n",
            "  libswresample   3.  7.100 /  3.  7.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './videos/uchimata_wall_trace.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:07.37, start: 0.000000, bitrate: 28 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 640x480, 24 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Unknown encoder 'libx264'\n"
          ]
        }
      ],
      "source": [
        "compressed_path = save_compressed_video(output_path)\n",
        "# # Show video\n",
        "# mp4 = open(compressed_path,'rb').read()\n",
        "# data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "# HTML(\"\"\"\n",
        "# <video width=400 controls>\n",
        "#       <source src=\"%s\" type=\"video/mp4\">\n",
        "# </video>\n",
        "# \"\"\" % data_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A few ideas from ChatGPT on how to compare trackings obtained with pose tracking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#There are many ways to compare tracings in a graph when tracking body joints using pose tracking. Here are some ideas and Python code suggestions:\n",
        "\n",
        "# 1. Euclidean distance: Calculate the Euclidean distance between each point of one tracing from the other (similar to calculating the distance between two points in math).\n",
        "\n",
        "# ```python\n",
        "import numpy as np\n",
        "\n",
        "# Assuming \"trace_a, trace_b\" are two 2D numpy arrays representing the two tracings\n",
        "def euclidean_distance(trace_a, trace_b):\n",
        "    return np.sqrt(np.sum((trace_a - trace_b) ** 2))\n",
        "```\n",
        "\n",
        "# 2. Dynamic time warping: This is a method that measures similarity between two sequences of feature vectors. It can be used to compare two motion trajectories by measuring the cumulative distance between the corresponding points in each of the trajectories.\n",
        "\n",
        "# ```python\n",
        "from tslearn.metrics import dtw\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
        "\n",
        "# Assuming \"trace_a, trace_b\" are two 2D numpy arrays representing the two tracings\n",
        "def dynamic_time_warping(trace_a, trace_b):\n",
        "    # Normalize the traces so they have zero-mean and unit variance\n",
        "    trace_a_norm = TimeSeriesScalerMeanVariance().fit_transform(trace_a)\n",
        "    trace_b_norm = TimeSeriesScalerMeanVariance().fit_transform(trace_b)\n",
        "\n",
        "    # Calculate the dynamic time warping distance between the normalized traces\n",
        "    return dtw(trace_a_norm, trace_b_norm)\n",
        "```\n",
        "\n",
        "# 3. Pearson correlation coefficient: This measures the linear correlation between two variables (tracings) by calculating the covariance between them divided by the product of their standard deviations.\n",
        "\n",
        "# ```python\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Assuming \"trace_a, trace_b\" are two 1D numpy arrays representing the two tracings\n",
        "def pearson_correlation(trace_a, trace_b):\n",
        "    # If the tracings have different lengths, interpolate the shorter one\n",
        "    x_len, y_len = len(trace_a), len(trace_b)\n",
        "    if x_len > y_len:\n",
        "        trace_a = np.interp(np.linspace(0, 1, y_len), np.linspace(0, 1, x_len), trace_a)\n",
        "    elif y_len > x_len:\n",
        "        trace_b = np.interp(np.linspace(0, 1, x_len), np.linspace(0, 1, y_len), trace_b)\n",
        "\n",
        "    # Calculate the Pearson correlation coefficient\n",
        "    return pearsonr(trace_a, trace_b)[0]\n",
        "```\n",
        "\n",
        "# 4. Cosine distance: This measures the angle between two vectors and is commonly used to compare the similarity of two trajectories.\n",
        "\n",
        "# ```python\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Assuming \"trace_a, trace_b\" are two 1D numpy arrays representing the two tracings\n",
        "def cosine_distance(trace_a, trace_b):\n",
        "    # If the tracings have different lengths, interpolate the shorter one\n",
        "    x_len, y_len = len(trace_a), len(trace_b)\n",
        "    if x_len > y_len:\n",
        "        trace_a = np.interp(np.linspace(0, 1, y_len), np.linspace(0, 1, x_len), trace_a)\n",
        "    elif y_len > x_len:\n",
        "        trace_b = np.interp(np.linspace(0, 1, x_len), np.linspace(0, 1, y_len), trace_b)\n",
        "\n",
        "    # Calculate the cosine distance\n",
        "    return cosine(trace_a, trace_b)\n",
        "\n",
        "#These are just a few examples of methods that can be used to compare tracings in a graph when tracking body joints using pose tracking. Depending on the specific problem you are trying to solve, there may be other methods that are more appropriate.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "explore",
      "language": "python",
      "name": "explore"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

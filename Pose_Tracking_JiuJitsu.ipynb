{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installing the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvQIfrzE25Hu",
        "outputId": "d62fa410-a300-45ce-e431-4e7c76ca9c97"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.19.3\n",
        "!pip install mediapipe\n",
        "!pip install opencv-python\n",
        "!pip install matplotlib\n",
        "!pip install natsorted\n",
        "!pip install pandas\n",
        "!pip install plotly"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up automatic reload for the scripts that will be written outside the jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UzUeDmrg4xGU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-23 09:36:38.312919: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-23 09:36:38.424121: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lucassoares/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-23 09:36:38.424137: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2023-02-23 09:36:39.037613: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lucassoares/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-23 09:36:39.037673: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/lucassoares/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
            "2023-02-23 09:36:39.037679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "from base64 import b64encode\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "from natsort import natsorted\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from IPython.display import HTML\n",
        "from typing import List # I don't think I need this!\n",
        "\n",
        "# Custom imports\n",
        "from pose_tracking_utils import *"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up all the necessary variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "VIDEO_PATH = \"./uchimata_wall.mp4\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMAKyQ893kSr",
        "outputId": "fca8834b-9067-4e39-ea50-a60077199030"
      },
      "source": [
        "Now we create the pose tracking video to visualize the joints and connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For webcam input:\n",
        "output_path = create_pose_tracking_video(\"./videos/uchimata_wall.mp4\")\n",
        "print(output_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compress the video and visualize it on jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compressed_path = save_compressed_video(output_path)\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the landmark 3D plot animation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Wk_RmGfWzg4",
        "outputId": "838fcb67-df47-41e2-b353-1e0d5a62771d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignoring empty camera frame.\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "pose_coords = []\n",
        "# For video input\n",
        "VIDEO_PATH = \"./videos/uchimata_wall.mp4\"\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "with mp_pose.Pose(\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as pose:\n",
        "  while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "      print(\"Ignoring empty camera frame.\")\n",
        "      # If loading a video, use 'break' instead of 'continue'.\n",
        "      break\n",
        "    # To improve performance, optionally mark the image as not writeable to\n",
        "    # pass by reference.\n",
        "    image.flags.writeable = False\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image)\n",
        "    mp_drawing.plot_landmarks_and_save(\n",
        "         results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS, plot_index=i)\n",
        "    # Draw the pose annotation on the image.\n",
        "    # image.flags.writeable = True\n",
        "    # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    # mp_drawing.draw_landmarks(\n",
        "    #     image,\n",
        "    #     results.pose_landmarks,\n",
        "    #     mp_pose.POSE_CONNECTIONS,\n",
        "    #     landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n",
        "    \n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    #cv2.imshow(\"Pose viz\",cv2.flip(image, 1))\n",
        "    if cv2.waitKey(1) & 0xFF == 27:\n",
        "      break\n",
        "    \n",
        "    i+=1\n",
        "    \n",
        "\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_22753/2278354264.py:20: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(os.path.join(directory, png_file))\n",
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (1000, 1000) to (1008, 1008) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ],
      "source": [
        "create_animation_from_png(folder=\".\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pose landmarks are this:\n",
        "\n",
        "Another list of pose landmarks in world coordinates. Each landmark consists of the following:\n",
        "\n",
        "x, y and z: Real-world 3D coordinates in meters with the origin at the center between hips.\n",
        "visibility: Identical to that defined in the corresponding\n",
        "\n",
        "[source](https://google.github.io/mediapipe/solutions/pose.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the reference plot against which we will set objectives to improve\n",
        "the desired movements. Like for example I am using olympiam judokas\n",
        "as my reference to improve my uchimata movement.\n",
        "\n",
        "(*This function and the interactive widget are not finished*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_reference_plot(reference_coords: List, body_part_index: int):\n",
        "    \"\"\"\n",
        "    Creates a reference plot for the desired motion\n",
        "    Args:\n",
        "        reference_coords (list): List of landmark objects with the x,y,z \n",
        "        coordinates.\n",
        "        body_part_index (integer): index of the desired body part to plot.\n",
        "    \"\"\"\n",
        "    body_part_coords = [reference_coords[i].landmark[body_part_index] for i in range(len(reference_coords))]\n",
        "    \n",
        "    # rest of the function code goes here\n",
        "\n",
        "\n",
        "left_foot_coords = create_reference_plot(pose_coords, 31)\n",
        "right_foot_coords = create_reference_plot(pose_coords, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Extract x, y, z coordinates from left and right foot\n",
        "x_coords = [coord.x for coord in left_foot_coords]\n",
        "y_coords = [coord.y for coord in left_foot_coords]\n",
        "z_coords = [coord.z for coord in left_foot_coords]\n",
        "\n",
        "x_coords_right = [coord.x for coord in right_foot_coords]\n",
        "y_coords_right = [coord.y for coord in right_foot_coords]\n",
        "z_coords_right = [coord.z for coord in right_foot_coords]\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_title('Foot Movement Over Time')\n",
        "\n",
        "def update(frame):\n",
        "    ax.clear()\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    ax.set_title('Foot Movement Over Time')\n",
        "    ax.scatter(x_coords[:frame], y_coords[:frame], z_coords[:frame])\n",
        "    ax.scatter(x_coords_right[:frame], y_coords_right[:frame], z_coords_right[:frame])\n",
        "\n",
        "# Create a slider for selecting the frame\n",
        "frame_slider = widgets.IntSlider(value=0, min=0, max=len(x_coords), step=1, description='Frame')\n",
        "\n",
        "# Define a function that updates the plot when the slider is changed\n",
        "def on_value_change(change):\n",
        "    frame = change['new']\n",
        "    update(frame)\n",
        "\n",
        "# Attach the function to the slider\n",
        "frame_slider.observe(on_value_change, names='value')\n",
        "\n",
        "# Display the slider and the plot\n",
        "display(widgets.VBox([frame_slider, fig.canvas]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we create a 3D plot animation for body parts (in this case for the feet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_coords = [coord.x for coord in left_foot_coords]\n",
        "y_coords = [coord.y for coord in left_foot_coords]\n",
        "z_coords = [coord.z for coord in left_foot_coords]\n",
        "\n",
        "x_coords_right = [coord.x for coord in right_foot_coords]\n",
        "y_coords_right = [coord.y for coord in right_foot_coords]\n",
        "z_coords_right = [coord.z for coord in right_foot_coords]\n",
        "\n",
        "# Create a 3D scatter plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.set_title('Foot Movement Over Time')\n",
        "\n",
        "def update(frame):\n",
        "    ax.clear()\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    ax.set_title('Foot Movement Over Time')\n",
        "    ax.scatter(x_coords[:frame], y_coords[:frame], z_coords[:frame])\n",
        "    ax.scatter(x_coords_right[:frame], y_coords_right[:frame], z_coords_right[:frame])\n",
        "\n",
        "ani = FuncAnimation(fig, update, frames=len(x_coords), interval=5)\n",
        "ani.save('animation.mp4', writer='ffmpeg', fps=30)\n",
        "#plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nicer looking landdmark plot of specific moments of the video, using plotly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "plot_landmarks(pose_coords[200].pose_landmarks,  mp_pose.POSE_CONNECTIONS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "explore",
      "language": "python",
      "name": "explore"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
